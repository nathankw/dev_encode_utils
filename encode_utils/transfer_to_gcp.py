# -*- coding: utf-8 -*-

###
# Â© 2018 The Board of Trustees of the Leland Stanford Junior University
# Nathaniel Watson
# nathankw@stanford.edu
# 2018-07-20
###

import datetime
import json
import os
import googleapiclient.discovery
#pip install google-api-python-client (details https://developers.google.com/api-client-library/python/)
# List of APIs that google-api-python-client can use at https://developers.google.com/api-client-library/python/apis/
# Storage Transfer API docs at https://developers.google.com/resources/api-libraries/documentation/storagetransfer/v1/python/latest/

#storagetransfer = googleapiclient.discovery.build('storagetransfer', 'v1')

#def copy_files_to_gcp(s3_bucket, s3_paths, gcp_bucket, gcp_project, description="", aws_creds=()):
#    """
#    See example at https://cloud.google.com/storage-transfer/docs/create-client and
#    https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/storage/transfer_service/aws_request.py.
#
#    Copies one or more files from AWS S3 storage to GCP storage by using the Google Storage
#    Transfer Service. The transfer is scheduled to run in upto 1 minute from the time
#    this method is called.
#
#    AWS Credentials are fetched from the environment via the variables `AWS_ACCESS_KEY_ID` and
#    `AWS_SECRET_ACCESS_KEY`, unless passed explicitly to the aws_creds argument.
#
#    Google credentials are fetched from the environment via the variable
#    GOOGLE_APPLICATION_CREDENTIALS.  This should be set to the JSON file provided to you
#    by the GCP Console when you create a service account; see
#    https://cloud.google.com/docs/authentication/getting-started for more details. Note that
#    the service account that you create must have at least the two roles below:
#
#      1) Project role with access level of Editor or greater.
#      2) Storage role with access level of Storage Object Creator or greater.
#
#    Note1: If this is the first time that you are using the Google Storage Transfer Service on
#    your GCP bucket, it won't work just yet as you'll get an error that reads:
#
#      Failed to obtain the location of the destination Google Cloud Storage (GCS) bucket due to
#      insufficient permissions.  Please verify that the necessary permissions have been granted.
#      (Google::Apis::ClientError)
#
#    To resolve this, I recommend that you go into the GCP Console and run a manual transfer there,
#    as this adds the missing permission that you need. I personaly don't know how to add it
#    otherwise, or even know what it is that's being added, but there you go!
#
#    Note2: If a file transfer doens't work (i.e. it doesn't exist in source bucket or incorrect
#    path provided), I'm not aware of a way to know that w/o explicitely having to inspect the GCP
#    bucket for presence/absence of the file. Even in the GCP Console, the Tranfer job stil shows as green
#    and doesn't indicate any sort of failure.
#
#    Args:
#        s3_bucket: `str`. The name of the AWS S3 bucket.
#        s3_paths: `list`. The paths to S3 objects in s3_bucket. Don't include leading '/' (it will
#            be removed if seen at the beginning anyways). Up to 1000 files can be transferred in a
#            given transfer job, per the Storage Transfer API transferJobs_ documentation.
#        gcp_bucket: `str`. The name of the GCP bucket.
#        gcp_project: `str`. The GCP project that is associated with gcp_bucket. Can be given
#            in either integer form  or the user-friendly name form (i.e. sigma-night-207122)
#        description: `str`. The description to show when querying transfers via the
#             Google Storage Transfer API, or via the GCP Console. May be left empty, in which
#             case the default description will be the value of the first S3 file name to transfer.
#        aws_creds: `tuple`. Ideally, your AWS credentials will be stored in the environment.
#            For additional flexability though, you can specify them here as well in the form
#            ``(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)``.
#
#    Returns:
#        `dict`: The JSON response representing the newly created transfer job.
#
#    .. _transferJobs: https://developers.google.com/resources/api-libraries/documentation/storagetransfer/v1/python/latest/storagetransfer_v1.transferJobs.html
#    """
#    #See example at https://cloud.google.com/storage-transfer/docs/create-client and
#    # https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/storage/transfer_service/aws_request.py.
#
#    s3_paths = list(set(s3_paths))
#    # The transferJobs() doc specifies not to include leading '/'.
#    s3_paths = [x.lstrip("/") for x in s3_paths]
#
#    if aws_creds:
#        AWS_ACCESS_KEY_ID = aws_creds[0]
#        AWS_SECRET_ACCESS_KEY = aws_creds[1]
#    else:
#        AWS_ACCESS_KEY_ID = os.environ["AWS_ACCESS_KEY_ID"]
#        AWS_SECRET_ACCESS_KEY = os.environ["AWS_SECRET_ACCESS_KEY"]
#
#    # Start transfer between 1 to 2 minutes from now.
#    now = datetime.datetime.now(tz=datetime.timezone.utc)
#    hour = now.hour
#    minute = now.minute
#
#    if not description:
#        # Default description is then first s3 object to transfer.
#        description = s3_paths[0]
#    params = {}
#    params["description"] = description
#    params["status"] = "ENABLED"
#    params["projectId"] = gcp_project
#    # Set transfer day (set to today). Note that if the start day is different from the end day,
#    # then a daily transfer will be set that ends on the end date. If no end date is set, a daily
#    # transfer job will run each day. So, need to avoid those last two scenarios and stick with a
#    # one-time transfer.
#    params["schedule"] = {
#        "scheduleStartDate": {
#            "year": now.year,
#            "month": now.month,
#            "day": now.day
#        },
#        "scheduleEndDate": {
#            "year": now.year,
#            "month": now.month,
#            "day": now.day
#        }
#    }
#    params["transferSpec"] = {
#        "awsS3DataSource": {
#            "bucketName": "pulsar-encode-assets",
#            "awsAccessKey": {
#                "accessKeyId": AWS_ACCESS_KEY_ID,
#                "secretAccessKey": AWS_SECRET_ACCESS_KEY
#            }
#        },
#        "gcsDataSink": {
#            "bucketName": gcp_bucket
#        },
#        "objectConditions": {
#            "includePrefixes": s3_paths
#        }
#    }
#
#    job = storagetransfer.transferJobs().create(body=params).execute() #dict
#    job_id = job["name"].split("/")[-1]
#    print("Created transfer job with ID {}: {}".format(job_id, json.dumps(job, indent=4)))
#    return job
#
##def get_transfers_from_job(gcp_project, transferjob_name):
##    """
##    Fetches descriptions in JSON format of any realized transfers under the specified transferJob.
##    These are called transferOperations in the Google Storage Transfer API terminology.
##
##    See Google API example at https://cloud.google.com/storage-transfer/docs/create-manage-transfer-program?hl=ja
##    in the section called "Check transfer operation status".
##    See API details at https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations.
##
##    Args:
##        gcp_project: `str`. The GCP project in which the transferJob specified by transferjob_name
##            was created. The underlying API call requires that this be specified, and it can be
##            given in either integer form  or the user-friendly name form (i.e. sigma-night-207122).
##        transferjob_name: `str`. The value of the `name` key in the dictionary that is returned by
##          copy_files_to_gcp().
##
##    Returns:
##        `list` of transferOperations belonging to the specified transferJob. This will be a list
##            of only a single element if the transferJob is a one-off transfer. But if this is a
##            repeating transferJob, then there could be several transferOperations in the list.
##    """
##    filt = {}
##    filt["project_id"] = gcp_project
##    filt["job_names"] = [transferjob_name]
##    query = storagetransfer.transferOperations().list(
##        name="transferOperations",
##        filter=json.dumps(filt))
##    return query.execute()["operations"]
##
##def get_transfer_status(gcp_project, transferjob_name):
##    """
##    Returns the transfer status of the first transferOperation that is returned for the given
##    transferJob. Thus, this function really only makes sense for one-off transferJobs that don't
##    repeat.
##
##    Note: if a transferJob attempts to transfer a non-existing file from the source bucket,
##    this has no effect on the transferOperation status (it will not cause a FAILED status).
##    Moreover, transferOperation status doesn't look at what files were and were not transferred and is
##    ony concerned with the execution status of the transferOperation job itself.
##
##    Args:
##        gcp_project: `str`. The GCP project in which the transferJob specified by transferjob_name
##            was created. The underlying API call requires that this be specified, and it can be
##            given in either integer form  or the user-friendly name form (i.e. sigma-night-207122).
##        transferjob_name: `str`. The value of the `name` key in the dictionary that is returned by
##          copy_files_to_gcp().
##    """
##    meta = get_transfers_from_job(gcp_project=gcp_project, transferjob_name=transferjob_name)[0]["metadata"]
##    return meta["status"]
##
